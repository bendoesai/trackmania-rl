{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmrl import get_environment\n",
    "from time import sleep\n",
    "from math import floor, sqrt\n",
    "#from tmrl.custom.custom_models import conv2d_out_dims, num_flat_features, mlp\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tmrl.config.config_constants as cfg\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paramters\n",
    "image_height = 64  #height of images\n",
    "image_width = 64   #width of images\n",
    "num_images = 4  #number of images (tmrl returns 4 to us)\n",
    "keep_prob = 0.5     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Trackmania_Critic(nn.Module):\n",
    "    def __init__(self, seed):\n",
    "        super(CNN_Trackmania_Critic, self).__init__()\n",
    "\n",
    "        self.IMAGE_HEIGHT = image_height\n",
    "        self.IMAGE_WIDTH = image_width\n",
    "        self.keep_prob = keep_prob\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        # convolutional layers\n",
    "        self.conv1 = nn.Conv2d(num_images, 24, kernel_size=8, stride=2)\n",
    "        self.conv2 = nn.Conv2d(24, 36, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(36, 48, kernel_size=4, stride=2)\n",
    "        self.conv4 = nn.Conv2d(48, 64, kernel_size=4, stride=2)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(keep_prob)\n",
    "\n",
    "        # fully-connected layers\n",
    "        self.fc1 = nn.Linear(64 * 1 * 1 + 9, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "        self.fc4 = nn.Linear(10, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        speed, gear, rpm, images, act1, act2 = x\n",
    "        images = torch.from_numpy(images).float().to(device)\n",
    "        speed = torch.from_numpy(speed).to(device)\n",
    "        gear = torch.from_numpy(gear).to(device)\n",
    "        rpm = torch.from_numpy(rpm).to(device)\n",
    "        act1 = torch.from_numpy(act1).to(device)\n",
    "        act2 = torch.from_numpy(act2).to(device)\n",
    "\n",
    "        x = F.tanh(self.conv1(images))\n",
    "        x = F.tanh(self.conv2(x))\n",
    "        x = F.tanh(self.conv3(x))\n",
    "        x = F.tanh(self.conv4(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.view(-1)\n",
    "        act1 = act1.view(-1)\n",
    "        act2 = act2.view(-1)\n",
    "        #print(x.size())\n",
    "        x = torch.cat((speed, gear, rpm, x, act1, act2), -1)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Trackmania_Critic(nn.Module):\n",
    "    def __init__(self, seed):\n",
    "        super(CNN_Trackmania_Critic, self).__init__()\n",
    "\n",
    "        self.IMAGE_HEIGHT = image_height\n",
    "        self.IMAGE_WIDTH = image_width\n",
    "        self.keep_prob = keep_prob\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "        # convolutional layers\n",
    "        self.conv1 = nn.Conv2d(num_images, 24, kernel_size=8, stride=2)\n",
    "        self.conv2 = nn.Conv2d(24, 36, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(36, 48, kernel_size=4, stride=2)\n",
    "        self.conv4 = nn.Conv2d(48, 64, kernel_size=4, stride=2)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(keep_prob)\n",
    "\n",
    "        # fully-connected layers\n",
    "        self.fc1 = nn.Linear(64 * 1 * 1 + 9, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "        self.fc4 = nn.Linear(10, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        speed, gear, rpm, images, act1, act2 = x\n",
    "        images = torch.from_numpy(images).float().to(device)\n",
    "        speed = torch.from_numpy(speed).to(device)\n",
    "        gear = torch.from_numpy(gear).to(device)\n",
    "        rpm = torch.from_numpy(rpm).to(device)\n",
    "        act1 = torch.from_numpy(act1).to(device)\n",
    "        act2 = torch.from_numpy(act2).to(device)\n",
    "\n",
    "        x = F.tanh(self.conv1(images))\n",
    "        x = F.tanh(self.conv2(x))\n",
    "        x = F.tanh(self.conv3(x))\n",
    "        x = F.tanh(self.conv4(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.view(-1)\n",
    "        act1 = act1.view(-1)\n",
    "        act2 = act2.view(-1)\n",
    "        #print(x.size())\n",
    "        x = torch.cat((speed, gear, rpm, x, act1, act2), -1)\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent - Used from HW7 for a DeepQNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNAgent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = CNN_Trackmania(seed).to(device)\n",
    "        self.qnetwork_target = CNN_Trackmania(seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        \"\"\" speed, gear, rpm, images, act1, act2 = state\n",
    "        images = torch.from_numpy(images).float().to(device)\n",
    "        speed = torch.from_numpy(speed).to(device)\n",
    "        gear = torch.from_numpy(gear).to(device)\n",
    "        rpm = torch.from_numpy(rpm).to(device)\n",
    "        act1 = torch.from_numpy(act1).to(device)\n",
    "        act2 = torch.from_numpy(act2).to(device)\n",
    "\n",
    "        next_speed, next_gear, next_rpm, next_images, next_act1, next_act2 = next_state\n",
    "        next_images = torch.from_numpy(next_images).float().to(device)\n",
    "        next_speed = torch.from_numpy(next_speed).to(device)\n",
    "        next_gear = torch.from_numpy(next_gear).to(device)\n",
    "        next_rpm = torch.from_numpy(next_rpm).to(device)\n",
    "        next_act1 = torch.from_numpy(next_act1).to(device)\n",
    "        next_act2 = torch.from_numpy(next_act2).to(device)\n",
    "\n",
    "        state = speed, gear, rpm, images, act1, act2\n",
    "        next_state = next_speed, next_gear, next_rpm, next_images, next_act1, next_act2  \"\"\"\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        '''stop coding here'''\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        #state = torch.from_numpy(state).unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local.forward(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return action_values.cpu().data.numpy()\n",
    "        else:\n",
    "            return [random.uniform(-1,1),random.uniform(-1,1),random.uniform(-1,1)]\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        # Obtain random minibatch of tuples from D\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        #print(next_states)\n",
    "        \n",
    "        ## Compute and minimize the loss\n",
    "        ### Extract next maximum estimated value from target network\n",
    "        print(next_states.shape)\n",
    "        max_q_targets_next = 0\n",
    "        for next_state in next_states:\n",
    "            print(next_state)\n",
    "            q_targets_next = self.qnetwork_target(next_state).detach().unsqueeze(1)\n",
    "            if q_targets_next > \n",
    "        \n",
    "        '''Your code here'''\n",
    "        ### Calculate target value from bellman equation\n",
    "        ### Make sure to include a dones calculation, where q_targets is left alone if dones is 0 \n",
    "        ### and is just the reward if dones is 1\n",
    "        \n",
    "        #if dones = 1, q_targets = 1. If dones is 0,\n",
    "        q_targets = rewards + (gamma*q_targets_next)*(1-dones)\n",
    "        \n",
    "        '''Stop coding here'''\n",
    "        ### Calculate expected value from local network\n",
    "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "        ### Loss calculation (we used Mean squared error)\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chris\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py:1956: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.54322565], dtype=float32) array([1.], dtype=float32)\n",
      " array([186.06776], dtype=float32)\n",
      " array([[[143, 142, 141, ..., 141, 141, 141],\n",
      "         [143, 143, 143, ..., 141, 142, 143],\n",
      "         [145, 144, 144, ..., 142, 143, 143],\n",
      "         ...,\n",
      "         [216, 216, 216, ..., 218, 217, 217],\n",
      "         [216, 216, 217, ..., 217, 218, 217],\n",
      "         [216, 217, 216, ..., 217, 217, 218]],\n",
      "\n",
      "        [[143, 142, 141, ..., 141, 141, 141],\n",
      "         [143, 143, 143, ..., 141, 142, 143],\n",
      "         [145, 144, 144, ..., 142, 143, 143],\n",
      "         ...,\n",
      "         [216, 216, 216, ..., 218, 217, 217],\n",
      "         [216, 216, 217, ..., 217, 218, 217],\n",
      "         [216, 217, 216, ..., 217, 218, 218]],\n",
      "\n",
      "        [[143, 142, 141, ..., 141, 141, 141],\n",
      "         [143, 143, 143, ..., 141, 142, 143],\n",
      "         [145, 144, 144, ..., 142, 143, 143],\n",
      "         ...,\n",
      "         [216, 216, 216, ..., 218, 217, 218],\n",
      "         [216, 216, 217, ..., 217, 217, 217],\n",
      "         [216, 217, 216, ..., 217, 218, 218]],\n",
      "\n",
      "        [[143, 142, 141, ..., 141, 141, 141],\n",
      "         [143, 143, 143, ..., 141, 142, 143],\n",
      "         [145, 144, 144, ..., 142, 143, 143],\n",
      "         ...,\n",
      "         [216, 216, 216, ..., 218, 217, 218],\n",
      "         [216, 215, 217, ..., 217, 217, 217],\n",
      "         [216, 217, 216, ..., 217, 217, 218]]], dtype=uint8)\n",
      " array([[ 0.5116084 ,  0.236738  , -0.49898732]], dtype=float32)\n",
      " array([[0.9655709 , 0.62043446, 0.8043319 ]], dtype=float32)]\n",
      "(64, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chris\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\core\\shape_base.py:121: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16368\\3988468429.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m#print(act)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mnext_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# step (rtgym ensures healthy time-steps)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#step agent and learn reward from given action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mrew\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16368\\3584621585.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16368\\3584621585.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, experiences, gamma)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mq_targets_next\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mq_targets_next\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqnetwork_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;34m'''Your code here'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chris\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16368\\575236699.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mspeed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrpm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mspeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 6)"
     ]
    }
   ],
   "source": [
    "\n",
    "#model = CNN_Trackmania(image_height, image_width, num_images, keep_prob)\n",
    "# Let us retrieve the TMRL Gymnasium environment.\n",
    "# The environment you get from get_environment() depends on the content of config.json\n",
    "env = get_environment()\n",
    "agent = CNNAgent(state_size=6,action_size=3, seed=0) #State: speed, gear, rpm, x (4 images), act1, act2\n",
    "                                                     #Action: speed, gear, rpm between -1 and 1\n",
    "sleep(3)  # just so we have time to focus the TM20 window after starting the script\n",
    "\n",
    "\n",
    "eps_start = 1\n",
    "eps_end=0.01\n",
    "eps_decay=0.995\n",
    "scores = []\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "eps = eps_start\n",
    "for i_episode in range(1, 2000):\n",
    "    score = 0 #initialize episode score to 0\n",
    "    obs, info = env.reset() #get state when lunar lander is restarted\n",
    "    for _ in range(20000):  # rtgym ensures this runs at 20Hz by default\n",
    "        #act = model(torch.from_numpy(obs[3]))  # compute action\n",
    "        #print(obs)\n",
    "        #print(obs)\n",
    "        #obs = np.asarray(obs).astype(np.float32)\n",
    "        act = agent.act(obs, eps)\n",
    "        #act = act.detach().numpy()\n",
    "        \n",
    "        act = np.array(act)\n",
    "        #print(act)\n",
    "        next_obs, rew, terminated, truncated, info = env.step(act)  # step (rtgym ensures healthy time-steps)\n",
    "        agent.step(obs, act, rew, next_obs, terminated) #step agent and learn reward from given action\n",
    "        obs = next_obs\n",
    "        score += rew\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    scores_window.append(score)       # save most recent score\n",
    "    scores.append(score)              # save most recent score\n",
    "    eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "    if np.mean(scores_window)>=200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "    print(\"Episode \" + str(i_episode) + \": \" + str(score))\n",
    "env.wait()  # rtgym-specific method to artificially 'pause' the environment when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
